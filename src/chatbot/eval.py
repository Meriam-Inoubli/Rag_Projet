import os
import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
from dotenv import load_dotenv
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
import ast  # Pour convertir les cha√Ænes en listes
import asyncio
from ragas import SingleTurnSample, EvaluationDataset, evaluate
from ragas.metrics import faithfulness, answer_relevancy, LLMContextPrecisionWithReference
from langchain.chat_models import ChatOpenAI

# Suppress symlinks warning on Windows
os.environ["HF_HUB_DISABLE_SYMLINKS_WARNING"] = "1"

# Charger les variables d'environnement depuis le fichier .env
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")

if not openai_api_key:
    st.error("La cl√© API OpenAI est manquante. Veuillez v√©rifier votre fichier .env.")
    st.stop()

# Configurer l'API OpenAI pour LangChain
evaluator_llm = ChatOpenAI(model="gpt-4", temperature=0, openai_api_key=openai_api_key)

# Charger un mod√®le d'embedding pour la similarit√© cosinus
@st.cache_resource
def load_model():
    return SentenceTransformer('paraphrase-MiniLM-L6-v2')

model = load_model()

def calculate_cosine_similarity(text1, text2):
    embeddings = model.encode([text1, text2])
    similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]
    return similarity

def calculate_precision_recall_f1(answer, reference):
    """
    Calcule les m√©triques de pr√©cision, rappel et F1-score.
    """
    answer_tokens = set(answer.lower().split())
    reference_tokens = set(reference.lower().split())

    true_positives = len(answer_tokens.intersection(reference_tokens))
    false_positives = len(answer_tokens - reference_tokens)
    false_negatives = len(reference_tokens - answer_tokens)

    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0
    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

    return precision, recall, f1

def convert_to_list(context):
    """
    Convertit une cha√Æne de caract√®res en liste si n√©cessaire.
    """
    if isinstance(context, str):
        if context.startswith("[") and context.endswith("]"):
            try:
                return ast.literal_eval(context)
            except (ValueError, SyntaxError):
                return [context]
        else:
            return [context]
    return context

async def evaluate_sample(sample):
    """
    √âvalue un √©chantillon avec Ragas et retourne les r√©sultats.
    """
    dataset = EvaluationDataset([sample])
    
    # Initialiser la m√©trique avec le LLM
    context_precision = LLMContextPrecisionWithReference(llm=evaluator_llm)

    try:
        # Obtenir l'√©valuation
        result = evaluate(
            dataset,
            metrics=[faithfulness, answer_relevancy],
        )

        # Convertir le r√©sultat en dictionnaire
        result_dict = result.to_dict()  # ‚úÖ Solution

        context_precision_score = await context_precision.single_turn_ascore(sample)

        return {
            "Faithfulness": result_dict["faithfulness"],  
            "Answer Relevancy": result_dict["answer_relevancy"],
            "Context Precision": context_precision_score
        }
    except Exception as e:
        return {"error": str(e)}

def  display_evaluation_page():
    st.title("üìä √âvaluation du syst√®me RAG")
    st.markdown("### √âvaluez les performances de votre syst√®me RAG en utilisant des m√©triques standards et sp√©cifiques.")

    # Charger les donn√©es d'√©valuation
    st.markdown("#### Charger les donn√©es d'√©valuation")
    uploaded_file = st.file_uploader("T√©l√©chargez un fichier CSV contenant les questions, les r√©ponses g√©n√©r√©es, les r√©ponses de r√©f√©rence et les contextes", type=["csv"])

    if uploaded_file is not None:
        evaluation_data = pd.read_csv(uploaded_file)
        evaluation_data["retrieved_contexts"] = evaluation_data["retrieved_contexts"].apply(convert_to_list)

        st.write("Aper√ßu des donn√©es charg√©es :")
        st.dataframe(evaluation_data.head())

        required_columns = ["user_input", "response", "reference", "retrieved_contexts"]
        if all(col in evaluation_data.columns for col in required_columns):
            if evaluation_data[required_columns].isnull().any().any():
                st.error("Le fichier CSV contient des valeurs manquantes. Veuillez v√©rifier les donn√©es.")
            else:
                st.success("Donn√©es charg√©es avec succ√®s !")

                sample_size = st.slider("S√©lectionnez le nombre de questions √† √©valuer", min_value=1, max_value=len(evaluation_data), value=5)
                sampled_data = evaluation_data.sample(n=sample_size, random_state=42)

                for index, row in sampled_data.iterrows():
                    st.markdown(f"---")
                    st.markdown(f"### Question : **{row['user_input']}**")

                    col1, col2 = st.columns(2)
                    with col1:
                        st.markdown("**R√©ponse g√©n√©r√©e :**")
                        st.write(row["response"])
                    with col2:
                        st.markdown("**R√©ponse de r√©f√©rence :**")
                        st.write(row["reference"])

                    similarity_score = calculate_cosine_similarity(row["reference"], row["response"])
                    precision, recall, f1 = calculate_precision_recall_f1(row["response"], row["reference"])

                    metrics_table = pd.DataFrame({
                        "M√©trique": ["Cosine Similarity", "Precision", "Recall", "F1 Score"],
                        "Valeur": [similarity_score, precision, recall, f1]
                    })

                    sample = SingleTurnSample(
                        user_input=row["user_input"],
                        reference=row["reference"],
                        retrieved_contexts=row["retrieved_contexts"],
                        response=row["response"]
                    )

                    with st.spinner("Calcul des m√©triques Ragas..."):
                        loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(loop)
                        ragas_metrics = loop.run_until_complete(evaluate_sample(sample))

                        if "error" in ragas_metrics:
                            st.error(f"Une erreur s'est produite : {ragas_metrics['error']}")
                        else:
                            col_table, col_graph = st.columns(2)

                            with col_table:
                                st.markdown("#### M√©triques")
                                st.table(metrics_table)

                            with col_graph:
                                st.markdown("#### M√©triques Ragas")
                                fig, ax = plt.subplots()
                                bars = ax.bar(ragas_metrics.keys(), ragas_metrics.values(), color=["blue", "green", "orange"])
                                ax.set_ylabel("Score")
                                ax.set_title("M√©triques Ragas")
                                ax.set_ylim(0, 1)

                                for bar in bars:
                                    height = bar.get_height()
                                    ax.annotate(f'{height:.2f}',
                                                xy=(bar.get_x() + bar.get_width() / 2, height),
                                                xytext=(0, 3),
                                                textcoords="offset points",
                                                ha='center', va='bottom')

                                st.pyplot(fig)
        else:
            st.error(f"Le fichier CSV doit contenir les colonnes suivantes : {required_columns}")
